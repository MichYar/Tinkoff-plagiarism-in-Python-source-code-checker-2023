# Tinkoff-plagiarism-in-Python-source-code-checker-2023
*Простите за плохой английский в англоязычном файле - я специально*

Это не совсем README, так как все спецификации указаны в ТЗ и не нарушаются. Это, скорее просто история разрабротки и пояснение работы алгоритма.

## История разработки данного алгоритма
Как сказано в экзаменационном задании, простейший способ высчитать разницу между текстами - будь они программами или нет - это **расстояние Левенштейна** (LD). Первая версия скрипта высчитывала его напрямую, но давайте подумаем на этот счёт.

Код может и не быть скопирован напрямую, но... кхм... *одолживший* его мог заменить имена переменных, функци, классов, и т.д. LD текстовых программ будет неправильной мерой схожести - чисто технически, пролграммы разные во многих, *многих* местах, но логика действия одна и та же.

Чтобы избежать этого, коды программ сначала были **токенизированы** и LD высчитывалось уже между последовательностями токенов - это не позволяет нам попадаться на простую замену имён идентификаторов. А затем из программ были удалены комментарии (имеющие свой отдельный токен) и docstrings (определены как строковые литералы, единственные в своей логической строке - т.е., docstring не является операндом), которые не влияют на логику программы, но обманывают текстовый (и токеновый, но гораздо меньше) счётчик LD.

LD работает, говоря по-простому, примерно так: 0 - коды одинаковы, большое число - коды разные. Но на меру схожести более удобно смотреть таким образом: 0 - коды разные, 1 - коды одинаковы. Формула, переводящая LD в такой формат, была найдена и вставлена в скрипт.

Но плагиат можно сокрыть и другим образом: к примеру, *перестановка* функций, классов, структур или базовых операторов - в тех случаях, когда программной логике это не вредит; и *мёртвый код* - неиспользуемый код, который просто занимает место и не используется в программе, или недостижимый код, который просто никогда не выполнится.

Проблема перестановки трудна и методы её решения во многих случаях ненадёжны: мы можем ошибиться и случайно испортить логику проверяющей программы, что приведёт к ещё большим ошибкам. Также существует метрика Дамерау-Левенштейна (DLD), которая помимо удалений, вставок и замен учитывает ещё и перестановки, но мы имеем дело с последовательностью токенов, а их порядок - ключевой момент в логике программы.

Проблема мёртвого кода также неоднозначна: даже реальные линтеры и оптимизирующие компиляторы иногда или недостаточно хороши, или недостаточно умны для выявления мёртвого кода. Были мысли насчёт удаления *очевидно недостижимого кода* - такого как кода, расположенного после `return`, `break`, `continue` (внутри их самой внутренней области обзора, конечно, иначе это может привести к ошибкам) или кода в идиомах вида `if False` или `if True ... else`. Я думаю, что это был бы интересный, но не всегда неошибающийся (к сожалению) модуль скрипта, но если у нас будет больше времени и, надеюсь, меньше ограничений на библиотеки, может, что-нибудь в этой области и получится - тогда это будет дополнительная экспериментальная функция.

## Рабочий алгоритм

В общем и целом, алгоритм работает следующим манером:

1. Считывание кода из текстовых файлов - так они не будут повреждены
2. Токенизация кода
3. Удаление комментариев и docstrings
4. Высчитывание LD между последовательностями токенов
5. Выдача меры схожести
